## The updated version of Subsection 2.2

Note that the bolded texts are newly added parts.

2.2 Delayed Feedback in CVR Prediction

Methods for tackling delayed feedback in CVR prediction fall into two categories: offline batch training [2, 14, 39] and online training [17, 36, 43]. In offline learning, DFM [2] is a pioneer, using two generalized linear models: one for predicting CVR and another for estimating delay time, under the assumption of an exponential delay distribution. NoDeF [40] builds on this by representing the delay distribution without a parametric assumption, while another approach utilizes importance weighting by setting a counterfactual deadline [39]. MM-DFM [14] treats CVR prediction under delayed feedback as a multi-task learning problem, incorporating multiple conversion labels across various observation intervals. convDF and nnDF [38] address delayed feedback through convex unbiased empirical risk estimators, based on time window and stationarity assumptions. ULC [32] corrects labels for negative samples using an auxiliary model trained on artificial data with counterfactual labeling. Addressing cascade delayed feedback, ECAD [46] models delays in both conversions and refunds. In online learning, methods like FN weighted, ES-DFM, DEFER, and DEFUSE tackle delayed feedback via diverse importance sampling techniques [3, 10, 17, 36]. CBDF [43] conceptualizes the challenge as a sequential decision making problem, employing a batched bandit. FTP [19] updates its model online, guided by a prophet model trained on offline data with confirmed labels. Some methods also utilize post-click behaviors like add-to-cart to refine CVR models [29, 35, **50**]. Our paper concentrates on optimizing offline pretraining for CVR prediction without relying on post-click behaviors other than actual conversions, formulating this as an unsupervised domain adaptation problem.
