## The updated version of Subsection 2.2

2.2 Delayed Feedback in CVR Prediction}
Methods for tackling delayed feedback in CVR prediction fall into two categories: offline batch training \cite{df1, df3, df9} and online training \cite{df2, df5, df6}. In offline learning, DFM \cite{df1} is a pioneer, using two generalized linear models: one for predicting CVR and another for estimating delay time, under the assumption of an exponential delay distribution. NoDeF \cite{df15} builds on this by representing the delay distribution without a parametric assumption, while another approach utilizes importance weighting by setting a counterfactual deadline \cite{df3}. MM-DFM \cite{df9} treats CVR prediction under delayed feedback as a multi-task learning problem, incorporating multiple conversion labels across various observation intervals. convDF and nnDF \cite{df12} address delayed feedback through convex unbiased empirical risk estimators, based on time window and stationarity assumptions. ULC \cite{df14} corrects labels for negative samples using an auxiliary model trained on artificial data with counterfactual labeling. Addressing cascade delayed feedback, ECAD \cite{df13} models delays in both conversions and refunds. In online learning, methods like FN weighted, ES-DFM, DEFER, and DEFUSE tackle delayed feedback via diverse importance sampling techniques \cite{df2, df6, df8, df11}. CBDF \cite{df5} conceptualizes the challenge as a sequential decision making problem, employing a batched bandit. FTP \cite{df7} updates its model online, guided by a prophet model trained on offline data with confirmed labels. Some methods also utilize post-click behaviors like add-to-cart to refine CVR models \cite{df4, df10}. Our paper concentrates on optimizing offline pretraining for CVR prediction without relying on post-click behaviors other than actual conversions, formulating this as an unsupervised domain adaptation problem.
